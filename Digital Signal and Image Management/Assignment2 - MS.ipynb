{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Michela Sessi 777760"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words con LBP (Local Binary Pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import feature\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo la funzione load_lbp_features che consente di caricare le immagini a partire da diverse cartelle relative alle diverse classi. La funzione permette di ottenere delle features basate su LBP (Local Binary Pattern). Una volta calcolato LBP si scompone la rappresentazione in blocchi sovrapposti e ad ogni scorrimento si calcola l'istogramma per ciascun blocco.\n",
    "\n",
    "La funzione ci restituisce quattro oggetti X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_lbp_features(base_path, \n",
    "                  maximages=500, \n",
    "                  extension='.jpg', \n",
    "                  Points=24, Radius=8, shift = 8, dims = 16):\n",
    "    \n",
    "    labels = []\n",
    "    features = []\n",
    "    for di,d in enumerate(sorted(os.listdir(base_path))):\n",
    "        for fi,f in enumerate(sorted(os.listdir(base_path + d + '/'))):\n",
    "            if f.endswith(extension) and fi<maximages:\n",
    "                image = cv.imread(base_path + d + '/' + f, 0)            \n",
    "                #lbp_features\n",
    "                image1 = feature.local_binary_pattern(image, P=Points, R=Radius, method='uniform')\n",
    "                feats = []\n",
    "                for i in range(0,image1.shape[0],shift):\n",
    "                    for j in range(0,image1.shape[1],shift):\n",
    "                        hist = np.bincount(image1[i:i+dims,j:j+dims].flatten().astype(int), minlength=Points+2)\n",
    "                        feats.append(hist)\n",
    "                cur_features = np.array(feats)\n",
    "            \n",
    "                features.append(cur_features)\n",
    "                labels.append(di)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.4, shuffle=True, random_state=1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel nostro esempio LBP viene eseguito Points=24 e Raggio=8. Il blocco un quadrato di lato 16 (dims) con sovrapposizione a scorrimento di 8 in 8 (shift)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_lbp_features (base_path = './Esercitazioni_04_notebook/Esercitazioni_04/classes/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le funzioni successive bow_features e norm_disct_as lavorano concatenatamente. Dopo aver normalizzato le features si crea un dizionario tramite k-means. Ogni feature di ogni immagine verrà associata ad uno dei centroidi trovati. Si ottiene quindi che le features divengono l'istogramma normalizzato delle bag of words per ogni immagine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_features(feats, dictionary, nwords):\n",
    "    quantized = dictionary.predict(feats)\n",
    "    t = np.bincount(quantized, minlength=nwords)\n",
    "    return t\n",
    "\n",
    "def norm_dict_as(X_train, X_test, nwords, normalize=True, eps=0.001):\n",
    "   \n",
    "    #normalizzazione features\n",
    "    X_train_stack = np.zeros((0,X_train[0].shape[1]), dtype=np.float32)\n",
    "    for t in X_train:\n",
    "        X_train_stack = np.concatenate((X_train_stack, t))\n",
    "        \n",
    "    if normalize:\n",
    "        X_train_mean = X_train_stack.mean(axis=0)\n",
    "        X_train_std = X_train_stack.std(axis=0)\n",
    "        X_train = [(t - X_train_mean + eps)/(X_train_std + eps) for t in X_train]\n",
    "        X_test = [(t - X_train_mean + eps)/(X_train_std + eps) for t in X_test]\n",
    "        X_train_stack = (X_train_stack - X_train_mean + eps)/(X_train_std + eps)\n",
    "        \n",
    "    #creazione dizionario    \n",
    "    dictionary = MiniBatchKMeans(n_clusters=nwords)\n",
    "    dictionary.fit(X_train_stack)\n",
    "    \n",
    "    #assegnamento words\n",
    "    X_train = [bow_features(f, dictionary, nwords) for f in X_train]\n",
    "    X_test = [bow_features(f, dictionary, nwords) for f in X_test]\n",
    "    X_train = [hist/hist.sum() for hist in X_train]\n",
    "    X_test = [hist/hist.sum() for hist in X_test]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troviamo quindi un dizionario con nwords = 300 e le features finali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2, X_test2 = norm_dict_as (X_train, X_test, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alleniamo una SVM 5 Folds Cross Validation a partire dei valori per i parametri C e gamma generici per poi ricercare più centratamente sui valori ottenuti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addestramento completato in 34.271s\n",
      "Migliore combinazione di parametri:\n",
      " C: 1\n",
      " gamma: 500\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [ 1, 5, 10, 50, 100, 500],\n",
    "          'gamma': [ 1, 5, 10, 50, 100, 500], } \n",
    "\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "t2 = time()\n",
    "clf = clf.fit(X_train2, y_train)\n",
    "print(\"Addestramento completato in %0.3fs\" % (time() - t2))\n",
    "\n",
    "print(\"Migliore combinazione di parametri:\")\n",
    "print(\" C: \"+str(clf.best_estimator_.C))\n",
    "print(\" gamma: \"+str(clf.best_estimator_.gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addestramento completato in 15.109s\n",
      "Migliore combinazione di parametri:\n",
      " C: 1.5\n",
      " gamma: 300\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [ 0.5, 1, 1.5, 2, 2.5],\n",
    "          'gamma': [ 300, 400, 500, 600, 700], } \n",
    "\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "t2 = time()\n",
    "clf = clf.fit(X_train2, y_train)\n",
    "print(\"Addestramento completato in %0.3fs\" % (time() - t2))\n",
    "\n",
    "print(\"Migliore combinazione di parametri:\")\n",
    "print(\" C: \"+str(clf.best_estimator_.C))\n",
    "print(\" gamma: \"+str(clf.best_estimator_.gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I valori migliori sono risultati essere quindi C=1.5 e gamma=300.\n",
    "Riportiamo i risultati sul test nel seguente report. \n",
    "A seguito la relativa matrice di confusione e accuracy= 0.7075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report di classificazione:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.76      0.72       199\n",
      "          1       0.74      0.65      0.69       201\n",
      "\n",
      "avg / total       0.71      0.71      0.71       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test2)\n",
    "\n",
    "print(\"Report di classificazione:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice di confusione:\n",
      "[[152  47]\n",
      " [ 70 131]]\n",
      "Accuratezza media= 0.7075\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrice di confusione:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"Accuratezza media= \" + str(accuracy_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
